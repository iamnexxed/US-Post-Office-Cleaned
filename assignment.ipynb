{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 name      orig_name state orig_county  established  \\\n",
      "2               AARON          AARON    MO       Bates       1895.0   \n",
      "5               AARON          AARON    GA     Bulloch       1909.0   \n",
      "7               AARON          AARON    SC    Anderson       1892.0   \n",
      "8          AARONSBURG  AARONSBURG(H)    PA      Centre       1792.0   \n",
      "11             ABADYL         ABADYL    MO   Christian       1895.0   \n",
      "...               ...            ...   ...         ...          ...   \n",
      "166129         ZURICH         ZURICH    MT      Blaine       1907.0   \n",
      "166131           ZYBA           ZYBA    KS      Sumner       1887.0   \n",
      "166133         ZYBACH         ZYBACH    TX     Wheeler       1910.0   \n",
      "166136           TRUE           TRUE    TX       Young       1894.0   \n",
      "166137  SIERRA BLANCA  SIERRA BLANCA    TX    Hudspeth       1882.0   \n",
      "\n",
      "        continuous  stamp_index  coordinates   latitude   longitude  \n",
      "2             True          2.0         True  38.422222  -94.154167  \n",
      "5             True          3.0         True  32.568220  -81.992340  \n",
      "7             True          6.0         True  34.577053  -82.623466  \n",
      "8             True          0.0         True  40.898967  -77.452199  \n",
      "11            True          4.0         True  37.038386  -92.947119  \n",
      "...            ...          ...          ...        ...         ...  \n",
      "166129        True          0.0         True  48.584330 -109.031700  \n",
      "166131        True          5.0         True  37.434740  -97.389210  \n",
      "166133        True          2.0         True  35.619210 -100.190700  \n",
      "166136        True          4.0         True  33.279830  -98.728670  \n",
      "166137        True          0.0         True  31.175930 -105.360000  \n",
      "\n",
      "[112490 rows x 10 columns]\n",
      "Non-convertible values in 'established' column:\n",
      "                  name            orig_name state     orig_county  \\\n",
      "80712         ELKRIDGE         ELKRIDGE (2)    MD          Howard   \n",
      "37044      MIAMI BEACH          MIAMI BEACH    FL            Dade   \n",
      "18486           ENGINE               ENGINE    MS         Neshoba   \n",
      "83431  FORT WASHINGTON  FORT WASHINGTON (2)    MD  Prince Georges   \n",
      "43810       PARK HILLS           PARK HILLS    MO    St. Francois   \n",
      "\n",
      "       established  continuous  stamp_index  coordinates   latitude  longitude  \n",
      "80712       2000.0        True          0.0         True  39.190580 -76.747780  \n",
      "37044       1999.0        True          0.0         True  25.784469 -80.132489  \n",
      "18486       1999.0       False          2.0         True  32.639026 -89.092281  \n",
      "83431       1996.0        True          0.0         True  38.730840 -76.991650  \n",
      "43810       1995.0        True          0.0         True  37.850758 -90.521844  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(\"postoffice.csv\")\n",
    "\n",
    "columns_to_remove = ['duration', 'alt_name', 'discontinued', 'county1', 'county2', 'county3',  'id', 'gnis_name', 'gnis_match', 'gnis_county', 'gnis_state', 'gnis_feature_id', 'gnis_feature_class', 'gnis_orig_name', 'gnis_orig_county', 'gnis_latitude', 'gnis_longitude', 'gnis_elev_in_m', 'gnis_dist']\n",
    "\n",
    "# Remove the specified columns\n",
    "df_cleaned = df.drop(columns=columns_to_remove)\n",
    "df_cleaned = df_cleaned.dropna(subset=['latitude', 'longitude', 'established'])\n",
    "\n",
    "\n",
    "# Keep only the current running offices\n",
    "#df_cleaned = df_cleaned[df_cleaned['duration'].isna()]\n",
    "\n",
    "# Calculate 'duration' based on 'established' column if it's missing\n",
    "#df_cleaned['duration'] = df_cleaned.apply(lambda row: (2021 - row['established']) if pd.isnull(row['duration']) else row['duration'], axis=1)\n",
    "\n",
    "print(df_cleaned)\n",
    "\n",
    "# Check for non-convertible values in 'established' column\n",
    "non_convertible_established = df_cleaned[df_cleaned['established'].isnull()]['established']\n",
    "print(\"Non-convertible values in 'established' column:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_cleaned_sorted = df_cleaned.sort_values(by='established', ascending=False)\n",
    "\n",
    "# Keep only the top 1000 recent post offices\n",
    "#df_top_1000_recent = df_cleaned_sorted.head(1000)\n",
    "\n",
    "df_top_1000_recent = df_cleaned_sorted\n",
    "\n",
    "# Check the first few rows of the resulting DataFrame\n",
    "print(df_top_1000_recent.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with NA values removed. Cleaned data saved to postoffice_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Convert 'duration' and 'established' columns to integers\n",
    "#df_top_1000_recent['duration'] = df_top_1000_recent['duration'].astype(int)\n",
    "df_top_1000_recent['established'] = df_top_1000_recent['established'].astype(int)\n",
    "\n",
    "\n",
    "# Write the cleaned data to a new CSV file\n",
    "df_top_1000_recent.to_csv(\"postoffice_cleaned.csv\", index=False)\n",
    "\n",
    "print(\"Rows with NA values removed. Cleaned data saved to postoffice_cleaned.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State-wise post office data saved to statewise_post_offices.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the cleaned CSV file\n",
    "df_cleaned = pd.read_csv(\"postoffice_cleaned.csv\")\n",
    "\n",
    "# Dictionary mapping short names to long names of states\n",
    "state_names_long = {\n",
    "    'AL': 'Alabama', 'AK': 'Alaska', 'AZ': 'Arizona', 'AR': 'Arkansas', 'CA': 'California',\n",
    "    'CO': 'Colorado', 'CT': 'Connecticut', 'DE': 'Delaware', 'FL': 'Florida', 'GA': 'Georgia',\n",
    "    'HI': 'Hawaii', 'ID': 'Idaho', 'IL': 'Illinois', 'IN': 'Indiana', 'IA': 'Iowa',\n",
    "    'KS': 'Kansas', 'KY': 'Kentucky', 'LA': 'Louisiana', 'ME': 'Maine', 'MD': 'Maryland',\n",
    "    'MA': 'Massachusetts', 'MI': 'Michigan', 'MN': 'Minnesota', 'MS': 'Mississippi', 'MO': 'Missouri',\n",
    "    'MT': 'Montana', 'NE': 'Nebraska', 'NV': 'Nevada', 'NH': 'New Hampshire', 'NJ': 'New Jersey',\n",
    "    'NM': 'New Mexico', 'NY': 'New York', 'NC': 'North Carolina', 'ND': 'North Dakota', 'OH': 'Ohio',\n",
    "    'OK': 'Oklahoma', 'OR': 'Oregon', 'PA': 'Pennsylvania', 'RI': 'Rhode Island', 'SC': 'South Carolina',\n",
    "    'SD': 'South Dakota', 'TN': 'Tennessee', 'TX': 'Texas', 'UT': 'Utah', 'VT': 'Vermont',\n",
    "    'VA': 'Virginia', 'WA': 'Washington', 'WV': 'West Virginia', 'WI': 'Wisconsin', 'WY': 'Wyoming'\n",
    "}\n",
    "\n",
    "# Invert the dictionary to get short names as keys and long names as values\n",
    "state_names_short = {v: k for k, v in state_names_long.items()}\n",
    "\n",
    "# Map short state names to long state names\n",
    "df_cleaned['state_longform'] = df_cleaned['state'].map(state_names_long)\n",
    "\n",
    "# Aggregate number of post offices per state\n",
    "state_post_offices_long = df_cleaned.groupby('state_longform').size().reset_index(name='num_post_offices')\n",
    "\n",
    "# Map long state names back to short state names\n",
    "state_post_offices_long['state'] = state_post_offices_long['state_longform'].map(state_names_short)\n",
    "\n",
    "# Concatenate short and long state names with aggregated number of post offices\n",
    "state_post_offices = pd.concat([state_post_offices_long[['state', 'state_longform', 'num_post_offices']], state_post_offices_long[['state', 'state_longform', 'num_post_offices']]])\n",
    "\n",
    "# Calculate the centroid of each state and save it to the CSV\n",
    "centroid_latitudes = []\n",
    "centroid_longitudes = []\n",
    "\n",
    "for state_longform in state_post_offices['state_longform']:\n",
    "    state_data = df_cleaned[df_cleaned['state_longform'] == state_longform]\n",
    "    centroid_latitude = state_data['latitude'].mean()\n",
    "    centroid_longitude = state_data['longitude'].mean()\n",
    "    centroid_latitudes.append(centroid_latitude)\n",
    "    centroid_longitudes.append(centroid_longitude)\n",
    "\n",
    "state_post_offices['centroid_latitude'] = centroid_latitudes\n",
    "state_post_offices['centroid_longitude'] = centroid_longitudes\n",
    "\n",
    "# Write the aggregated data to a new CSV file\n",
    "state_post_offices.to_csv(\"statewise_post_offices.csv\", index=False)\n",
    "\n",
    "print(\"State-wise post office data saved to statewise_post_offices.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
