{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-convertible values in 'established' column:\n",
      "28676\n",
      "                      name            orig_name state     orig_county  \\\n",
      "80712             ELKRIDGE         ELKRIDGE (2)    MD          Howard   \n",
      "37044          MIAMI BEACH          MIAMI BEACH    FL            Dade   \n",
      "83431      FORT WASHINGTON  FORT WASHINGTON (2)    MD  Prince Georges   \n",
      "3309   BATESBURG-LEESVILLE  BATESBURG-LEESVILLE    SC       Lexington   \n",
      "43810           PARK HILLS           PARK HILLS    MO    St. Francois   \n",
      "\n",
      "       established  continuous  stamp_index  coordinates  duration   latitude  \\\n",
      "80712       2000.0        True          0.0         True      21.0  39.190580   \n",
      "37044       1999.0        True          0.0         True      22.0  25.784469   \n",
      "83431       1996.0        True          0.0         True      25.0  38.730840   \n",
      "3309        1995.0        True          0.0         True      26.0  33.916532   \n",
      "43810       1995.0        True          0.0         True      26.0  37.850758   \n",
      "\n",
      "       longitude  \n",
      "80712 -76.747780  \n",
      "37044 -80.132489  \n",
      "83431 -76.991650  \n",
      "3309  -81.513437  \n",
      "43810 -90.521844  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(\"postoffice.csv\")\n",
    "\n",
    "columns_to_remove = ['alt_name', 'discontinued', 'county1', 'county2', 'county3',  'id', 'gnis_name', 'gnis_match', 'gnis_county', 'gnis_state', 'gnis_feature_id', 'gnis_feature_class', 'gnis_orig_name', 'gnis_orig_county', 'gnis_latitude', 'gnis_longitude', 'gnis_elev_in_m', 'gnis_dist']\n",
    "\n",
    "# Remove the specified columns\n",
    "df_cleaned = df.drop(columns=columns_to_remove)\n",
    "df_cleaned = df_cleaned.dropna(subset=['latitude', 'longitude', 'established'])\n",
    "# Keep only the current running offices\n",
    "df_cleaned = df_cleaned[df_cleaned['duration'].isna()]\n",
    "# Calculate 'duration' based on 'established' column if it's missing\n",
    "df_cleaned['duration'] = df_cleaned.apply(lambda row: (2021 - row['established']) if pd.isnull(row['duration']) else row['duration'], axis=1)\n",
    "\n",
    "# Check for non-convertible values in 'established' column\n",
    "non_convertible_established = df_cleaned[df_cleaned['established'].isnull()]['established']\n",
    "print(\"Non-convertible values in 'established' column:\")\n",
    "print(len(df_cleaned))\n",
    "\n",
    "df_cleaned_sorted = df_cleaned.sort_values(by='established', ascending=False)\n",
    "\n",
    "# Keep only the top 1000 recent post offices\n",
    "df_top_1000_recent = df_cleaned_sorted.head(1000)\n",
    "\n",
    "# Check the first few rows of the resulting DataFrame\n",
    "print(df_top_1000_recent.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with NA values removed. Cleaned data saved to postoffice_cleaned.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mt/rp9xph9j5nvb8zt52znkhhr00000gn/T/ipykernel_46903/515352458.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_top_1000_recent['duration'] = df_top_1000_recent['duration'].astype(int)\n",
      "/var/folders/mt/rp9xph9j5nvb8zt52znkhhr00000gn/T/ipykernel_46903/515352458.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_top_1000_recent['established'] = df_top_1000_recent['established'].astype(int)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Convert 'duration' and 'established' columns to integers\n",
    "df_top_1000_recent['duration'] = df_top_1000_recent['duration'].astype(int)\n",
    "df_top_1000_recent['established'] = df_top_1000_recent['established'].astype(int)\n",
    "\n",
    "\n",
    "# Write the cleaned data to a new CSV file\n",
    "df_top_1000_recent.to_csv(\"postoffice_cleaned_top1000.csv\", index=False)\n",
    "\n",
    "print(\"Rows with NA values removed. Cleaned data saved to postoffice_cleaned.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State-wise post office data saved to statewise_post_offices.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the cleaned CSV file\n",
    "df_cleaned = pd.read_csv(\"postoffice_cleaned_top1000.csv\")\n",
    "\n",
    "# Dictionary mapping short names to long names of states\n",
    "state_names_long = {\n",
    "    'AL': 'Alabama', 'AK': 'Alaska', 'AZ': 'Arizona', 'AR': 'Arkansas', 'CA': 'California',\n",
    "    'CO': 'Colorado', 'CT': 'Connecticut', 'DE': 'Delaware', 'FL': 'Florida', 'GA': 'Georgia',\n",
    "    'HI': 'Hawaii', 'ID': 'Idaho', 'IL': 'Illinois', 'IN': 'Indiana', 'IA': 'Iowa',\n",
    "    'KS': 'Kansas', 'KY': 'Kentucky', 'LA': 'Louisiana', 'ME': 'Maine', 'MD': 'Maryland',\n",
    "    'MA': 'Massachusetts', 'MI': 'Michigan', 'MN': 'Minnesota', 'MS': 'Mississippi', 'MO': 'Missouri',\n",
    "    'MT': 'Montana', 'NE': 'Nebraska', 'NV': 'Nevada', 'NH': 'New Hampshire', 'NJ': 'New Jersey',\n",
    "    'NM': 'New Mexico', 'NY': 'New York', 'NC': 'North Carolina', 'ND': 'North Dakota', 'OH': 'Ohio',\n",
    "    'OK': 'Oklahoma', 'OR': 'Oregon', 'PA': 'Pennsylvania', 'RI': 'Rhode Island', 'SC': 'South Carolina',\n",
    "    'SD': 'South Dakota', 'TN': 'Tennessee', 'TX': 'Texas', 'UT': 'Utah', 'VT': 'Vermont',\n",
    "    'VA': 'Virginia', 'WA': 'Washington', 'WV': 'West Virginia', 'WI': 'Wisconsin', 'WY': 'Wyoming'\n",
    "}\n",
    "\n",
    "# Invert the dictionary to get short names as keys and long names as values\n",
    "state_names_short = {v: k for k, v in state_names_long.items()}\n",
    "\n",
    "# Map short state names to long state names\n",
    "df_cleaned['state_longform'] = df_cleaned['state'].map(state_names_long)\n",
    "\n",
    "# Aggregate number of post offices per state\n",
    "state_post_offices_long = df_cleaned.groupby('state_longform').size().reset_index(name='num_post_offices')\n",
    "\n",
    "# Map long state names back to short state names\n",
    "state_post_offices_long['state'] = state_post_offices_long['state_longform'].map(state_names_short)\n",
    "\n",
    "# Concatenate short and long state names with aggregated number of post offices\n",
    "state_post_offices = pd.concat([state_post_offices_long[['state', 'state_longform', 'num_post_offices']], state_post_offices_long[['state', 'state_longform', 'num_post_offices']]])\n",
    "\n",
    "# Calculate the centroid of each state and save it to the CSV\n",
    "centroid_latitudes = []\n",
    "centroid_longitudes = []\n",
    "\n",
    "for state_longform in state_post_offices['state_longform']:\n",
    "    state_data = df_cleaned[df_cleaned['state_longform'] == state_longform]\n",
    "    centroid_latitude = state_data['latitude'].mean()\n",
    "    centroid_longitude = state_data['longitude'].mean()\n",
    "    centroid_latitudes.append(centroid_latitude)\n",
    "    centroid_longitudes.append(centroid_longitude)\n",
    "\n",
    "state_post_offices['centroid_latitude'] = centroid_latitudes\n",
    "state_post_offices['centroid_longitude'] = centroid_longitudes\n",
    "\n",
    "# Write the aggregated data to a new CSV file\n",
    "state_post_offices.to_csv(\"statewise_post_offices_top1000.csv\", index=False)\n",
    "\n",
    "print(\"State-wise post office data saved to statewise_post_offices.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
